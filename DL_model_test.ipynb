{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "274906c2",
   "metadata": {},
   "source": [
    "## Demonstration: CRC model\n",
    "The following code is a small example from the original DL module for proteomic analysis of colorectal tissue samples. \n",
    "\n",
    "\n",
    "--> The .mzmL or .d (bruker) files produced from a mass spectrometry experiment were further analysed using DIA-NN software. The output was .csv files which contained information about predicted parameters like precursor intensity, precurson quantity, Retention time etc. \n",
    "\n",
    "--> While in the original project the aim was to compare the protein expression including peptide analysis; between tumor and normal tissue dataset, in this project, I have only compared the normalised peptide intensities.\n",
    "\n",
    "--> I have only one (sample) file for each category, which is reflected in model performance.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3baea46",
   "metadata": {},
   "source": [
    "### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eaaf3e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_tuner import HyperModel, Hyperband"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74ba723",
   "metadata": {},
   "source": [
    "### 2. Load and Pre-process the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0119652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    tumor_data = pd.read_csv('tumor.csv')\n",
    "    normal_data = pd.read_csv('normal.csv')\n",
    "\n",
    "    columns_of_interest = [\n",
    "        'Protein.Names',\n",
    "        'Modified.Sequence',  # Peptide sequences\n",
    "        'Precursor.Quantity', # Peptide intensities\n",
    "        'Precursor.Charge',\n",
    "        'RT',\n",
    "    ]\n",
    "\n",
    "## Clean Protein--->[sometimes protein names come in semi-colon seperated format, so taking first protein name to standardise]\n",
    "\n",
    "    def clean_protein_names(protein_name):\n",
    "        return protein_name.split(';')[0] if pd.notnull(protein_name) else protein_name\n",
    "    \n",
    "## Normalised Peptide Intensities--->[each protein has several peptides, their intensity is normalised so that there is no bias from number of peptides per protein]\n",
    "\n",
    "    def normalize_intensity(data):\n",
    "        return data.groupby('Protein.Names')['Precursor.Quantity'].apply(lambda x: x / x.sum()).reset_index(drop=True)\n",
    "\n",
    "    def filter_and_normalize(data):\n",
    "        filtered_data = data[columns_of_interest].copy()\n",
    "        filtered_data['Protein.Names'] = filtered_data['Protein.Names'].apply(clean_protein_names)\n",
    "        filtered_data['Normalized.Intensity'] = normalize_intensity(filtered_data)\n",
    "        return filtered_data\n",
    "\n",
    "    filtered_tumor_data = filter_and_normalize(tumor_data)\n",
    "    filtered_normal_data = filter_and_normalize(normal_data)\n",
    "\n",
    "    filtered_tumor_data.to_csv(\"filtered_data_tumor.csv\", index=False)\n",
    "    filtered_normal_data.to_csv(\"filtered_data_normal.csv\", index=False)\n",
    "\n",
    "    print('Filtered tumor data saved to: filtered_data_tumor.csv')\n",
    "    print('Filtered normal data saved to: filtered_data_normal.csv')\n",
    "    \n",
    "    return filtered_tumor_data, filtered_normal_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f003f9e6",
   "metadata": {},
   "source": [
    "### 3. Prepare the features and labels for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6defe829",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_features_and_labels(tumor_data, normal_data):\n",
    "    tumor_data['label'] = 1\n",
    "    normal_data['label'] = 0\n",
    "\n",
    "    data = pd.concat([tumor_data, normal_data], ignore_index=True)\n",
    "\n",
    "    X_numeric = data[['Normalized.Intensity', 'RT']].values\n",
    "    y = data['label'].values\n",
    "\n",
    "    one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "    categorical_features = one_hot_encoder.fit_transform(data[['Precursor.Charge']])\n",
    "\n",
    "    X = np.concatenate([X_numeric, categorical_features], axis=1)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724ec60e",
   "metadata": {},
   "source": [
    "### 4. Model building using Keras Tuner for hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41b8f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1],)),\n",
    "        Dense(hp.Int('units_layer1', min_value=32, max_value=128, step=32), activation='relu'),\n",
    "        Dropout(hp.Float('dropout_layer1', 0.2, 0.5, step=0.1)),\n",
    "        Dense(hp.Int('units_layer2', min_value=16, max_value=64, step=16), activation='relu'),\n",
    "        Dropout(hp.Float('dropout_layer2', 0.2, 0.5, step=0.1)),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65c97f0",
   "metadata": {},
   "source": [
    "### 5. Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286a935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X_train, X_test, y_train, y_test):\n",
    "    tuner = Hyperband(\n",
    "        build_model,\n",
    "        objective='val_accuracy',\n",
    "        max_epochs=10,\n",
    "        factor=3,\n",
    "        directory='my_dir',\n",
    "        project_name='hyperparameter_tuning'\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "    tuner.search(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=50,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    print(f\"Best units_layer1: {best_hps.get('units_layer1')}\")\n",
    "    print(f\"Best dropout_layer1: {best_hps.get('dropout_layer1')}\")\n",
    "    print(f\"Best units_layer2: {best_hps.get('units_layer2')}\")\n",
    "    print(f\"Best dropout_layer2: {best_hps.get('dropout_layer2')}\")\n",
    "    print(f\"Best learning rate: {best_hps.get('learning_rate')}\")\n",
    "\n",
    "    best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "    loss, accuracy = best_model.evaluate(X_test, y_test)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    best_model.save('best_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53569f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered tumor data saved to: filtered_data_tumor.csv\n",
      "Filtered normal data saved to: filtered_data_normal.csv\n",
      "Reloading Tuner from my_dir\\hyperparameter_tuning\\tuner0.json\n",
      "Best units_layer1: 32\n",
      "Best dropout_layer1: 0.30000000000000004\n",
      "Best units_layer2: 16\n",
      "Best dropout_layer2: 0.2\n",
      "Best learning rate: 0.01\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3033 - loss: 0.8139  \n",
      "Test Accuracy: 0.3193\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tumor_data, normal_data = load_and_preprocess_data()\n",
    "    X, y = prepare_features_and_labels(tumor_data, normal_data)\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    train_and_evaluate(X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997c7ade",
   "metadata": {},
   "source": [
    "### 6. Perform K-Fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a9864b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sharm\\Desktop\\UNI\\bioinformatics\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy for Fold 1: 0.7048\n",
      "Validation Accuracy for Fold 2: 0.6687\n",
      "Validation Accuracy for Fold 3: 0.7590\n",
      "Validation Accuracy for Fold 4: 0.6687\n",
      "Validation Accuracy for Fold 5: 0.6848\n",
      "Mean Validation Accuracy: 0.6972\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def perform_cross_validation(X, y, n_splits=5):\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    val_scores = []\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=0)\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_scaled), 1):\n",
    "        X_train_fold, X_val_fold = X_scaled[train_index], X_scaled[val_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "        # Model architecture\n",
    "        fold_model = Sequential([\n",
    "            Dense(64, input_shape=(X_train_fold.shape[1],), activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        # Compile the model\n",
    "        fold_model.compile(optimizer='adam',\n",
    "                           loss='binary_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "        # Train the model\n",
    "        fold_model.fit(X_train_fold, y_train_fold,\n",
    "                       validation_data=(X_val_fold, y_val_fold),\n",
    "                       epochs=50,\n",
    "                       batch_size=32,\n",
    "                       callbacks=[early_stopping],\n",
    "                       verbose=0)  # Use verbose=0 to hide training output during CV\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        val_loss, val_accuracy = fold_model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "        print(f'Validation Accuracy for Fold {fold}: {val_accuracy:.4f}')\n",
    "        val_scores.append(val_accuracy)\n",
    "\n",
    "    # Calculate mean accuracy across all folds\n",
    "    mean_val_accuracy = np.mean(val_scores)\n",
    "    print(f'Mean Validation Accuracy: {mean_val_accuracy:.4f}')\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    perform_cross_validation(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
